{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishekv23/EE954-DL/blob/main/Q3_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07b36e45",
      "metadata": {
        "id": "07b36e45"
      },
      "source": [
        "# Implement a CNN backbone model using pytorch. (total 40 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35b5d3b5",
      "metadata": {
        "id": "35b5d3b5"
      },
      "source": [
        "### a. Build a small CNN model consisting of\n",
        "- 5 convolution Layers with each layer:\n",
        "1. Convolution layer\n",
        "2. ReLU activation\n",
        "3. Max pooling layer\n",
        "\n",
        "(10 Marks )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "763d26d0",
      "metadata": {
        "id": "763d26d0"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b6fc919",
      "metadata": {
        "id": "5b6fc919"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, transforms\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f9b9a6a",
      "metadata": {
        "id": "9f9b9a6a"
      },
      "outputs": [],
      "source": [
        "def FMNIST_DataLoader():\n",
        "  #load MNIST data\n",
        "  transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.5,), (0.5,))\n",
        "  ])\n",
        "  train_data = datasets.FashionMNIST(root='data', train=True, download=True, transform=transform)\n",
        "  test_data = datasets.FashionMNIST(root='data', train=False, download=True, transform=transform)\n",
        "  train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "  test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "  return train_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1f430c9",
      "metadata": {
        "id": "c1f430c9"
      },
      "outputs": [],
      "source": [
        "print(train_data.train_labels)\n",
        "print(train_data.data[0])\n",
        "train_data.train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56f71308",
      "metadata": {
        "id": "56f71308"
      },
      "outputs": [],
      "source": [
        "x_train = train_data.data/255\n",
        "x_test = test_data.data/255\n",
        "y_train = train_data.targets\n",
        "y_test = test_data.targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3854dd10",
      "metadata": {
        "id": "3854dd10"
      },
      "outputs": [],
      "source": [
        "(x_train, x_valid) = x_train[10000:], x_train[:10000]\n",
        "(y_train, y_valid) = y_train[10000:], y_train[:10000]\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1018741e",
      "metadata": {
        "id": "1018741e"
      },
      "outputs": [],
      "source": [
        "a = np.array(y_train)\n",
        "b = np.zeros((a.size, a.max() + 1))\n",
        "b[np.arange(a.size), a] = 1\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff0ecdaa",
      "metadata": {
        "id": "ff0ecdaa"
      },
      "outputs": [],
      "source": [
        "a1 = np.array(y_valid)\n",
        "b1 = np.zeros((a1.size, a1.max() + 1))\n",
        "b1[np.arange(a1.size), a1] = 1\n",
        "y_train = torch.tensor(b)\n",
        "y_valid = torch.tensor(b1)\n",
        "y_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f69566b9",
      "metadata": {
        "id": "f69566b9"
      },
      "outputs": [],
      "source": [
        "x_train[0:1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b2ba50d",
      "metadata": {
        "id": "3b2ba50d"
      },
      "outputs": [],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42a544c8",
      "metadata": {
        "id": "42a544c8"
      },
      "outputs": [],
      "source": [
        "class CNN_Model(nn.Module):\n",
        "\n",
        "    #define init function\n",
        "    def __init__(self):\n",
        "        super(CNN_Model, self).__init__()\n",
        "\n",
        "        #input size. 28 channel - 1,\n",
        "        #32 - output channel from this layer,\n",
        "        #square kerner of size 3.\n",
        "        #stride defaults to 1 and padding to 0.\n",
        "        self.layer1_conv = nn.Conv2d(1, 16, 2, padding=1)\n",
        "\n",
        "        self.layer1_activ = nn.ReLU()\n",
        "        self.layer1_maxpool = nn.MaxPool2d(2, stride=1) #2x2 kernel\n",
        "\n",
        "        #layer 2, input size\n",
        "        self.layer2_conv = nn.Conv2d(16, 32, 2, padding=1)\n",
        "        self.layer2_activ = nn.ReLU()\n",
        "        self.layer2_maxpool = nn.MaxPool2d(2, stride=1)\n",
        "        #layer 3\n",
        "        self.layer3_conv = nn.Conv2d(32, 64, 2, padding=1)\n",
        "        self.layer3_activ = nn.ReLU()\n",
        "        self.layer3_maxpool = nn.MaxPool2d(2, stride=2) #14x14x64\n",
        "        #layer 4\n",
        "        self.layer4_conv = nn.Conv2d(64, 128, 2, padding=1)\n",
        "        self.layer4_activ = nn.ReLU()\n",
        "        self.layer4_maxpool = nn.MaxPool2d(2, stride=2) #7x7x128\n",
        "        #layer 5\n",
        "        self.layer5_conv = nn.Conv2d(128, 128, 2, padding=1)\n",
        "        self.layer5_activ = nn.ReLU()\n",
        "        self.layer5_maxpool = nn.MaxPool2d(2, stride=2) #4x4x256\n",
        "        #Dense layer, input size=256*\n",
        "\n",
        "        self.dense1 = nn.Linear(2048, 20)\n",
        "        #output layer\n",
        "        self.out = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #pass data x through 1st layer\n",
        "        x = self.layer1_conv(x)\n",
        "        #print(f\"shape of data after layer1_conv: {x.shape}\")\n",
        "        x = self.layer1_activ(x)\n",
        "        x = self.layer1_maxpool(x)\n",
        "        #print(f\"shape of data after layer1_maxpool: {x.shape}\")\n",
        "\n",
        "        #pass data through 2nd layer\n",
        "        x = self.layer2_conv(x)\n",
        "        #print(f\"shape of data after layer2_conv: {x.shape}\")\n",
        "        x = self.layer2_activ(x)\n",
        "        x = self.layer2_maxpool(x)\n",
        "        #print(f\"shape of data after layer2_maxpool: {x.shape}\")\n",
        "\n",
        "        #pass data through 3rd layer\n",
        "        x = self.layer3_conv(x)\n",
        "        #print(f\"shape of data after layer3_conv: {x.shape}\")\n",
        "        x = self.layer3_activ(x)\n",
        "        x = self.layer3_maxpool(x)\n",
        "        #print(f\"shape of data after layer3_maxpool: {x.shape}\")\n",
        "\n",
        "        #pass data through 4th layer\n",
        "        x = self.layer4_conv(x)\n",
        "        #print(f\"shape of data after layer4_conv: {x.shape}\")\n",
        "        x = self.layer4_activ(x)\n",
        "        x = self.layer4_maxpool(x)\n",
        "        #print(f\"shape of data after layer4_maxpool: {x.shape}\")\n",
        "\n",
        "        #pass data through 5th layer\n",
        "        x = self.layer5_conv(x)\n",
        "        #print(f\"shape of data after layer5_conv: {x.shape}\")\n",
        "        x = self.layer5_activ(x)\n",
        "        x = self.layer5_maxpool(x)\n",
        "        #print(f\"shape of data after layer5_maxpool: {x.shape}\")\n",
        "        #print(len(x[1]))\n",
        "        #print(len(x[1][1]))\n",
        "        #print(len(x[1][1][1]))\n",
        "\n",
        "        #flatten\n",
        "        x = x.view(-1, 128*4*4)\n",
        "\n",
        "        #x = self.flatten = torch.flatten(x)\n",
        "        #This method will return flattened data that will be passed to Dense layer from Q2\n",
        "        #following 2 lines will be commented after testing.\n",
        "        #print(f\"shape of data after flatten: {x.shape}\")\n",
        "        x = self.dense1(x)\n",
        "        x = self.out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5081fc35",
      "metadata": {
        "id": "5081fc35"
      },
      "outputs": [],
      "source": [
        "#Returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1)\n",
        "random_data = torch.rand((32, 1, 28, 28))\n",
        "\n",
        "cnn_model = CNN_Model()\n",
        "\n",
        "output_data = cnn_model(random_data)\n",
        "print (output_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13f88473",
      "metadata": {
        "id": "13f88473"
      },
      "outputs": [],
      "source": [
        "tainable_param = 0\n",
        "\n",
        "for name, param in cnn_model.named_parameters():\n",
        "    print(name, param.numel())\n",
        "    trainable_param += param.numel()\n",
        "\n",
        "print(f\"Total trainable parameters {trainable_param}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96a22562",
      "metadata": {
        "id": "96a22562"
      },
      "outputs": [],
      "source": [
        "for name, param in cnn_model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fb40cba",
      "metadata": {
        "id": "3fb40cba"
      },
      "source": [
        "### b. Experiment with different kernel size, number of kernel each layer (10 Marks)\n",
        "(keep number of filter same in each layer, double it in each layer etc) and settle with a combination which performs the best for the given problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e7507aa",
      "metadata": {
        "id": "0e7507aa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f13a7db3",
      "metadata": {
        "id": "f13a7db3"
      },
      "outputs": [],
      "source": [
        "#Train the model\n",
        "def cnn_model_train(train_dataloader, cnn_model, loss_func, optimizer):\n",
        "    train_data_size = len(train_dataloader.dataset)\n",
        "    #set the model to training mode\n",
        "    cnn_model.train()\n",
        "\n",
        "    for batch, (x_train, y_train) in enumerate(train_dataloader):\n",
        "      batch = batch+1\n",
        "      y_predict = cnn_model(x_train)\n",
        "      loss = loss_func(y_predict, y_train)\n",
        "\n",
        "      #backpropagate the prediction loss\n",
        "      loss.backward()\n",
        "      #adjust the parameters\n",
        "      optimizer.step()\n",
        "      #to reset the gradients of model parameters. Gradients by default add up;\n",
        "      #to prevent double-counting, we explicitly zero them at each iteration.\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #printout training metrics after batch of 100\n",
        "      if batch % 100 ==0:\n",
        "        loss = loss.item()\n",
        "        print(f\"Train loss: {loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb166d30",
      "metadata": {
        "id": "bb166d30"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "114ab6ae",
      "metadata": {
        "id": "114ab6ae"
      },
      "outputs": [],
      "source": [
        "#Test the model\n",
        "def cnn_model_test(test_dataloader, cnn_model, loss_func):\n",
        "    #set the model to evaluation (important for BN and Dropout layers)\n",
        "    cnn_model.eval()\n",
        "    num_batches = len(test_dataloader)\n",
        "\n",
        "    #initialize\n",
        "    test_loss, correct = 0, 0\n",
        "    #ensure that no grad are computed during test mode\n",
        "    with torch.no_grad():\n",
        "        for batch, (x_valid, y_valid) in enumerate(test_dataloader):\n",
        "          batch = batch+1\n",
        "          predict = cnn_model(x_valid)\n",
        "          test_loss = loss_func(predict, y_valid)\n",
        "          if batch % 100 ==0:\n",
        "            test_loss = test_loss/num_batches\n",
        "            print(f\"Test loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b36deaf",
      "metadata": {
        "scrolled": false,
        "id": "8b36deaf"
      },
      "outputs": [],
      "source": [
        "#\n",
        "cnn_model = CNN_Model()\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.001)\n",
        "train_dataloader, test_dataloader = FMNIST_DataLoader()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZZj5TAN5Vtab"
      },
      "id": "ZZj5TAN5Vtab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "16347005",
      "metadata": {
        "id": "16347005"
      },
      "source": [
        "### c. Try different weight initialization methods (random, Xavier, He) (5 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54275cb4",
      "metadata": {
        "id": "54275cb4"
      },
      "outputs": [],
      "source": [
        "# for block is for unit testing only NOT BE EXECUTED IN ACTUAL RUN\n",
        "for i, (x,y) in enumerate(train_dataloader):\n",
        "    print(i)\n",
        "    #print(x[0])\n",
        "    #print(y[0])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab435fd5",
      "metadata": {
        "id": "ab435fd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f45280f4-5b30-4070-c8db-6513620f2e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "Test loss: 0.0014774209121242166\n",
            "Test loss: 0.0008170984801836312\n",
            "Test loss: 0.0017778313485905528\n",
            "epoch 2\n",
            "Test loss: 0.0014774209121242166\n",
            "Test loss: 0.0008170984801836312\n",
            "Test loss: 0.0017778313485905528\n"
          ]
        }
      ],
      "source": [
        "epochs = 2\n",
        "for i in range(epochs):\n",
        "  print(f\"epoch {i+1}\")\n",
        "  cnn_model_train(train_dataloader, cnn_model, loss_func, optimizer)\n",
        "  cnn_model_test(test_dataloader, cnn_model, loss_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a97a475",
      "metadata": {
        "id": "3a97a475"
      },
      "source": [
        "### d. After extracting feature from CNN model use MLP for classification (15 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f24922f5",
      "metadata": {
        "id": "f24922f5"
      },
      "outputs": [],
      "source": [
        "#(use code from question 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2c6ad82",
      "metadata": {
        "id": "f2c6ad82"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}